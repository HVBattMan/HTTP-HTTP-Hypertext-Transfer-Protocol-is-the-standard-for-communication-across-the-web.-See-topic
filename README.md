# HTTP-HTTP-Hypertext-Transfer-Protocol-is-the-standard-for-communication-across-the-web.-See-topic
Fuel Cells - Hydrogen Fuel Cell Description &amp; Advantages ... https://www.hydrogenics.com/.../fuel-cells DefinitionFunctionsAppearanceMechanismDetailsApplications A fuel cell is a device that converts chemical potential energy (energy stored in molecular bonds) into electrical energy. A PEM (Proton Exchange Membrane) cell uses hydrogen gas (H2) and oxygen gas (O2) as fuel. The products of the reaction in the cell are water, electricity, and heat. This is a big improvement over internal combustion engines, coal burning power plants, and nuclear power plants, all of which produce harmful by-products. See more on hydrogenics.com
Hydrogen + Oxygen = Electricity + Water Vapor

Cathode: O2 + 4H+ + 4e– → 2H2O
Anode: 2H2 → 4H+ + 4e–
Overall: 2H2 + O2 → 2H2O
A fuel cell is a device that converts chemical potential energy (energy stored in molecular bonds) into electrical
Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 

















@@ -150,7 +150,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -166,7 +166,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -179,7 +179,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 



@@ -194,7 +194,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 



@@ -205,7 +205,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





@@ -233,7 +233,7 @@ <h2 id='experiment-4'>Experiment 4: Rotating the perceptive field</h2>





<p><figure> 





<p><figure> 





    <img src="figures/rotation.png" style="width: 448px"> 





    <img src="figures/rotation.png" style="width: 448px"> 





  <figcaption> 





  <figcaption> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 



@@ -258,41 +258,45 @@ <h3>Neural Networks and Self-Organisation</h3>





<h3>Swarm robotics</h3> 





<h3>Swarm robotics</h3> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<h2 id='discussion'>Discussion</h2> 





<h2 id='discussion'>Discussion</h2> 





<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts.</p> 





<h3>Embryogenetic modeling</h3> 















<p><figure> 





<p><figure> 





    <video loop autoplay playsinline muted> 





    <video loop autoplay playsinline muted> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Regeneration-capable 2-headed planarian <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite>, the creature that inspired this work. 





Regeneration-capable 2-headed planarian, the creature that inspired this work <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite> 





<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 









<h3>Engineering and machine learning</h3> 









<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts. We therefore conjecture this kind of modeling may be used for designing reliable, self-organising agents. On the more theoretical machine learning front, we show an instance of a decentralized model able to accomplish remarkably complex tasks. We believe this direction to be opposite to the more traditional global modeling used in the majority of contemporary work in the deep learning field, and we hope this work to be an inspiration to explore more decentralized learning modeling.</p> 





</d-article> 





</d-article> 





<d-appendix> 





<d-appendix> 





<h3>Acknowledgments</h3> 





<h3>Acknowledgments</h3> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 

















<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<h3>Author Contributions</h3> 





<h3>Author Contributions</h3> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and supervised Ettore, who prototyped the model and designed the training regime for the first versions of the experiments. Finally, Alex added some key contributions to the model and the training regime, and performed the final versions of the experiments, shown in the article.</p> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Ettore designed and performed most of the experiments for this work. Alex supervisioned the entire process and contributed extensively to the later stages of development by performing experiments and refining the model.</p> 

















<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. The actual work was triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange with him.</p> 





<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. Alex’s model proposal and this work were triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange between Alex and Micheal.</p> 

















<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 





<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 

















<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<h3>Implementation details</h3> 





<h3>Implementation details</h3> 





<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 



















<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 





<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 

















<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 





<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 

















<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 





<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 



















<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 















<d-footnote-list></d-footnote-list> 





<d-footnote-list></d-footnote-list> 





<d-citation-list></d-citation-list> 





<d-citation-list></d-citation-list> 





<d-appendix> 





<d-appendix> 

1 contents.html 













@@ -9,7 +9,6 @@ <h3>Contents</h3>





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





    </ul> 





    </ul> 





    <div><a href="#implementation">Implementation</a></div> 









    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





  </nav> 





  </nav> 





10 demo.html 













@@ -230,6 +230,10 @@





  width: 200px; 





  width: 200px; 





  height: 16px; 





  height: 16px; 





} 





} 



















.hint a { 









  font-size: 90%; 









} 





</style> 





</style> 

















<div class="l-body-outset grid" id="demo"> 





<div class="l-body-outset grid" id="demo"> 





@@ -268,8 +272,8 @@





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





        </div> 





        </div> 





        <div> 





        <div style="white-space: nowrap;"> 





            Rotation <span id="rotationLabel"></span><span class="hint">&nbsp;&nbsp;<a href="#experiment-4">(experiment 4)</a></span> 





            Rotation <span id="rotationLabel"></span>&nbsp;<span class="hint"><a href="#experiment-4">(experiment 4)</a></span> 





            <br> 





            <br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





        </div> 





        </div> 



@@ -294,7 +298,7 @@





            </span> 





            </span> 





        </div> 





        </div> 





        <div id="colab-hero-div"> 





        <div id="colab-hero-div"> 





          <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





        </div> 





        </div> 





    </div> 





    </div> 





</div> 





</div> 





43 public/index.html 













@@ -453,6 +453,10 @@ <h1>Growing Neural Cellular Automata</h1>





  width: 200px; 





  width: 200px; 





  height: 16px; 





  height: 16px; 





} 





} 



















.hint a { 









  font-size: 90%; 









} 





</style> 





</style> 

















<div class="l-body-outset grid" id="demo"> 





<div class="l-body-outset grid" id="demo"> 





@@ -491,8 +495,8 @@ <h1>Growing Neural Cellular Automata</h1>





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





        </div> 





        </div> 





        <div> 





        <div style="white-space: nowrap;"> 





            Rotation <span id="rotationLabel"></span><span class="hint">&nbsp;&nbsp;<a href="#experiment-4">(experiment 4)</a></span> 





            Rotation <span id="rotationLabel"></span>&nbsp;<span class="hint"><a href="#experiment-4">(experiment 4)</a></span> 





            <br> 





            <br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





        </div> 





        </div> 



@@ -517,7 +521,7 @@ <h1>Growing Neural Cellular Automata</h1>





            </span> 





            </span> 





        </div> 





        </div> 





        <div id="colab-hero-div"> 





        <div id="colab-hero-div"> 





          <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





        </div> 





        </div> 





    </div> 





    </div> 





</div> 





</div> 



@@ -544,7 +548,6 @@ <h3>Contents</h3>





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





    </ul> 





    </ul> 





    <div><a href="#implementation">Implementation</a></div> 









    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





  </nav> 





  </nav> 





@@ -663,7 +666,7 @@ <h2 id='experiment-1'>Experiment 1: Learning to Grow</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 

















@@ -699,7 +702,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -715,7 +718,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -728,7 +731,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 



@@ -743,7 +746,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 



@@ -754,7 +757,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





@@ -782,7 +785,7 @@ <h2 id='experiment-4'>Experiment 4: Rotating the perceptive field</h2>





<p><figure> 





<p><figure> 





    <img src="figures/rotation.png" style="width: 448px"> 





    <img src="figures/rotation.png" style="width: 448px"> 





  <figcaption> 





  <figcaption> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 



@@ -807,41 +810,45 @@ <h3>Neural Networks and Self-Organisation</h3>





<h3>Swarm robotics</h3> 





<h3>Swarm robotics</h3> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<h2 id='discussion'>Discussion</h2> 





<h2 id='discussion'>Discussion</h2> 





<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts.</p> 





<h3>Embryogenetic modeling</h3> 















<p><figure> 





<p><figure> 





    <video loop autoplay playsinline muted> 





    <video loop autoplay playsinline muted> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Regeneration-capable 2-headed planarian <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite>, the creature that inspired this work. 





Regeneration-capable 2-headed planarian, the creature that inspired this work <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite> 





<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 









<h3>Engineering and machine learning</h3> 









<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts. We therefore conjecture this kind of modeling may be used for designing reliable, self-organising agents. On the more theoretical machine learning front, we show an instance of a decentralized model able to accomplish remarkably complex tasks. We believe this direction to be opposite to the more traditional global modeling used in the majority of contemporary work in the deep learning field, and we hope this work to be an inspiration to explore more decentralized learning modeling.</p> 





</d-article> 





</d-article> 





<d-appendix> 





<d-appendix> 





<h3>Acknowledgments</h3> 





<h3>Acknowledgments</h3> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 

















<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<h3>Author Contributions</h3> 





<h3>Author Contributions</h3> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and supervised Ettore, who prototyped the model and designed the training regime for the first versions of the experiments. Finally, Alex added some key contributions to the model and the training regime, and performed the final versions of the experiments, shown in the article.</p> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Ettore designed and performed most of the experiments for this work. Alex supervisioned the entire process and contributed extensively to the later stages of development by performing experiments and refining the model.</p> 

















<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. The actual work was triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange with him.</p> 





<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. Alex’s model proposal and this work were triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange between Alex and Micheal.</p> 

















<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 





<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 

















<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<h3>Implementation details</h3> 





<h3>Implementation details</h3> 





<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 



















<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 





<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 

















<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 





<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 

















<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 





<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 



















<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 















<d-footnote-list></d-footnote-list> 





<d-footnote-list></d-footnote-list> 





<d-citation-list></d-citation-list> 





<d-citation-list></d-citation-list> 





<d-appendix> 

Given a version number MAJOR.MINOR.PATCH, increment the:
MAJOR version when you make incompatible API changes,
MINOR version when you add functionality in a backwards compatible manner, and
PATCH version when you make backwards compatible bug fixes.
Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.

Introduction
In the world of software management there exists a dreaded place called “dependency hell.” The bigger your system grows and the more packages you integrate into your software, the more likely you are to find yourself, one day, in this pit of despair.
In systems with many dependencies, releasing new package versions can quickly become a nightmare. If the dependency specifications are too tight, you are in danger of version lock (the inability to upgrade a package without having to release new versions of every dependent package). If dependencies are specified too loosely, you will inevitably be bitten by version promiscuity (assuming compatibility with more future versions than is reasonable). Dependency hell is where you are when version lock and/or version promiscuity prevent you from easily and safely moving your project forward.
As a solution to this problem, I propose a simple set of rules and requirements that dictate how version numbers are assigned and incremented. These rules are based on but not necessarily limited to pre-existing widespread common practices in use in both closed and open-source software. For this system to work, you first need to declare a public API. This may consist of documentation or be enforced by the code itself. Regardless, it is important that this API be clear and precise. Once you identify your public API, you communicate changes to it with specific increments to your version number. Consider a version format of X.Y.Z (Major.Minor.Patch). Bug fixes not affecting the API increment the patch version, backwards compatible API additions/changes increment the minor version, and backwards incompatible API changes increment the major version.
I call this system “Semantic Versioning.” Under this scheme, version numbers and the way they change convey meaning about the underlying code and what has been modified from one version to the next.

Semantic Versioning Specification (SemVer)
The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in RFC 2119.

Software using Semantic Versioning MUST declare a public API. This API could be declared in the code itself or exist strictly in documentation. However it is done, it SHOULD be precise and comprehensive.

A normal version number MUST take the form X.Y.Z where X, Y, and Z are non-negative integers, and MUST NOT contain leading zeroes. X is the major version, Y is the minor version, and Z is the patch version. Each element MUST increase numerically. For instance: 1.9.0 -> 1.10.0 -> 1.11.0.

Once a versioned package has been released, the contents of that version MUST NOT be modified. Any modifications MUST be released as a new version.

Major version zero (0.y.z) is for initial development. Anything MAY change at any time. The public API SHOULD NOT be considered stable.

Version 1.0.0 defines the public API. The way in which the version number is incremented after this release is dependent on this public API and how it changes.

Patch version Z (x.y.Z | x > 0) MUST be incremented if only backwards compatible bug fixes are introduced. A bug fix is defined as an internal change that fixes incorrect behavior.

Minor version Y (x.Y.z | x > 0) MUST be incremented if new, backwards compatible functionality is introduced to the public API. It MUST be incremented if any public API functionality is marked as deprecated. It MAY be incremented if substantial new functionality or improvements are introduced within the private code. It MAY include patch level changes. Patch version MUST be reset to 0 when minor version is incremented.

Major version X (X.y.z | X > 0) MUST be incremented if any backwards incompatible changes are introduced to the public API. It MAY also include minor and patch level changes. Patch and minor version MUST be reset to 0 when major version is incremented.

A pre-release version MAY be denoted by appending a hyphen and a series of dot separated identifiers immediately following the patch version. Identifiers MUST comprise only ASCII alphanumerics and hyphens [0-9A-Za-z-]. Identifiers MUST NOT be empty. Numeric identifiers MUST NOT include leading zeroes. Pre-release versions have a lower precedence than the associated normal version. A pre-release version indicates that the version is unstable and might not satisfy the intended compatibility requirements as denoted by its associated normal version. Examples: 1.0.0-alpha, 1.0.0-alpha.1, 1.0.0-0.3.7, 1.0.0-x.7.z.92, 1.0.0-x-y-z.–.

Build metadata MAY be denoted by appending a plus sign and a series of dot separated identifiers immediately following the patch or pre-release version. Identifiers MUST comprise only ASCII alphanumerics and hyphens [0-9A-Za-z-]. Identifiers MUST NOT be empty. Build metadata MUST be ignored when determining version precedence. Thus two versions that differ only in the build metadata, have the same precedence. Examples: 1.0.0-alpha+001, 1.0.0+20130313144700, 1.0.0-beta+exp.sha.5114f85, 1.0.0+21AF26D3—-117B344092BD.

Precedence refers to how versions are compared to each other when ordered.
Precedence MUST be calculated by separating the version into major, minor, patch and pre-release identifiers in that order (Build metadata does not figure into precedence).
Precedence is determined by the first difference when comparing each of these identifiers from left to right as follows: Major, minor, and patch versions are always compared numerically.
Example: 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1.
When major, minor, and patch are equal, a pre-release version has lower precedence than a normal version:
Example: 1.0.0-alpha < 1.0.0.
Precedence for two pre-release versions with the same major, minor, and patch version MUST be determined by comparing each dot separated identifier from left to right until a difference is found as follows:
Identifiers consisting of only digits are compared numerically.
Identifiers with letters or hyphens are compared lexically in ASCII sort order.
Numeric identifiers always have lower precedence than non-numeric identifiers.
A larger set of pre-release fields has a higher precedence than a smaller set, if all of the preceding identifiers are equal.
Example: 1.0.0-alpha < 1.0.0-alpha.1 < 1.0.0-alpha.beta < 1.0.0-beta < 1.0.0-beta.2 < 1.0.0-beta.11 < 1.0.0-rc.1 < 1.0.0.

Backus–Naur Form Grammar for Valid SemVer Versions
<valid semver> ::= <version core>
                 | <version core> "-" <pre-release>
                 | <version core> "+" <build>
                 | <version core> "-" <pre-release> "+" <build>

<version core> ::= <major> "." <minor> "." <patch>

<major> ::= <numeric identifier>

<minor> ::= <numeric identifier>

<patch> ::= <numeric identifier>

<pre-release> ::= <dot-separated pre-release identifiers>

<dot-separated pre-release identifiers> ::= <pre-release identifier>
                                          | <pre-release identifier> "." <dot-separated pre-release identifiers>

<build> ::= <dot-separated build identifiers>

<dot-separated build identifiers> ::= <build identifier>
                                    | <build identifier> "." <dot-separated build identifiers>

<pre-release identifier> ::= <alphanumeric identifier>
                           | <numeric identifier>

<build identifier> ::= <alphanumeric identifier>
                     | <digits>

<alphanumeric identifier> ::= <non-digit>
                            | <non-digit> <identifier characters>
                            | <identifier characters> <non-digit>
                            | <identifier characters> <non-digit> <identifier characters>

<numeric identifier> ::= "0"
                       | <positive digit>
                       | <positive digit> <digits>

<identifier characters> ::= <identifier character>
                          | <identifier character> <identifier characters>

<identifier character> ::= <digit>
                         | <non-digit>

<non-digit> ::= <letter>
              | "-"

<digits> ::= <digit>
           | <digit> <digits>

<digit> ::= "0"
          | <positive digit>

<positive digit> ::= "1" | "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9"

<letter> ::= "A" | "B" | "C" | "D" | "E" | "F" | "G" | "H" | "I" | "J"
           | "K" | "L" | "M" | "N" | "O" | "P" | "Q" | "R" | "S" | "T"
           | "U" | "V" | "W" | "X" | "Y" | "Z" | "a" | "b" | "c" | "d"
           | "e" | "f" | "g" | "h" | "i" | "j" | "k" | "l" | "m" | "n"
           | "o" | "p" | "q" | "r" | "s" | "t" | "u" | "v" | "w" | "x"
           | "y" | "z"

Why Use Semantic Versioning?
This is not a new or revolutionary idea. In fact, you probably do something close to this already. The problem is that “close” isn’t good enough. Without compliance to some sort of formal specification, version numbers are essentially useless for dependency management. By giving a name and clear definition to the above ideas, it becomes easy to communicate your intentions to the users of your software. Once these intentions are clear, flexible (but not too flexible) dependency specifications can finally be made.
A simple example will demonstrate how Semantic Versioning can make dependency hell a thing of the past. Consider a library called “Firetruck.” It requires a Semantically Versioned package named “Ladder.” At the time that Firetruck is created, Ladder is at version 3.1.0. Since Firetruck uses some functionality that was first introduced in 3.1.0, you can safely specify the Ladder dependency as greater than or equal to 3.1.0 but less than 4.0.0. Now, when Ladder version 3.1.1 and 3.2.0 become available, you can release them to your package management system and know that they will be compatible with existing dependent software.
As a responsible developer you will, of course, want to verify that any package upgrades function as advertised. The real world is a messy place; there’s nothing we can do about that but be vigilant. What you can do is let Semantic Versioning provide you with a sane way to release and upgrade packages without having to roll new versions of dependent packages, saving you time and hassle.
If all of this sounds desirable, all you need to do to start using Semantic Versioning is to declare that you are doing so and then follow the rules. Link to this website from your README so others know the rules and can benefit from them.

FAQ

How should I deal with revisions in the 0.y.z initial development phase?
The simplest thing to do is start your initial development release at 0.1.0 and then increment the minor version for each subsequent release.

How do I know when to release 1.0.0?
If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you’re worrying a lot about backwards compatibility, you should probably already be 1.0.0.

Doesn’t this discourage rapid development and fast iteration?
Major version zero is all about rapid development. If you’re changing the API every day you should either still be in version 0.y.z or on a separate development branch working on the next major version.

If even the tiniest backwards incompatible changes to the public API require a major version bump, won’t I end up at version 42.0.0 very rapidly?
You need to install Java2 sdk, and setup JAVA_HOME enviromental varible except for OS X. I assume that OS X's JAVA_HOME is reported by calling /usr/libexec/java_home.
This done please proceed with:
ruby setup.rb config
ruby setup.rb setup
# (in Unix)
sudo ruby setup.rb install
or
# (in win32)
ruby setup.rb install
How to test
On Windows based machines:
cd test
ruby test.rb
On Unix based machines plese see test/readme.unix. You need to set LD_LIBRARY_PATH environmental variable to run rjb.
Notice for opening non-ASCII 7bit filename
If you'll plan to open the non-ascii character named file by Java class through Rjb, it may require to set LC_ALL environment variable in you sciprt.
For example in Rails, set above line in production.rb as your environment:This is a question of responsible development and foresight. Incompatible changes should not be introduced lightly to software that has a lot of dependent code. The cost that must be incurred to upgrade can be significant. Having to bump major versions to release incompatible changes means you’ll think through the impact of your changes, and evaluate the cost/benefit ratio involved.

Documenting the entire public API is too much work!
It is your responsibility as a professional developer to properly document software that is intended for use by others. Managing software complexity is a hugely important part of keeping a project efficient, and that’s hard to do if nobody knows how to use your software, or what methods are safe to call. In the long run, Semantic Versioning, and the insistence on a well defined public API can keep everyone and everything running smoothly.

What do I do if I accidentally release a backwards incompatible change as a minor version?
As soon as you realize that you’ve broken the Semantic Versioning spec, fix the problem and release a new minor version that corrects the problem and restores backwards compatibility. Even under this circumstance, it is unacceptable to modify versioned releases. If it’s appropriate, document the offending version and inform your users of the problem so that they are aware of the offending version.

What should I do if I update my own dependencies without changing the public API?
That would be considered compatible since it does not affect the public API. Software that explicitly depends on the same dependencies as your package should have their own dependency specifications and the author will notice any conflicts. Determining whether the change is a patch level or minor level modification depends on whether you updated your dependencies in order to fix a bug or introduce new functionality. I would usually expect additional code for the latter instance, in which case it’s obviously a minor level increment.

What if I inadvertently alter the public API in a way that is not compliant with the version number change (i.e. the code incorrectly introduces a major breaking change in a patch release)?
Use your best judgment. If you have a huge audience that will be drastically impacted by changing the behavior back to what the public API intended, then it may be best to perform a major version release, even though the fix could strictly be considered a patch release. Remember, Semantic Versioning is all about conveying meaning by how the version number changes. If these changes are important to your users, use the version number to inform them.

How should I handle deprecating functionality?
Deprecating existing functionality is a normal part of software development and is often required to make forward progress. When you deprecate part of your public API, you should do two things: (1) update your documentation to let users know about the change, (2) issue a new minor release with the deprecation in place. Before you completely remove the functionality in a new major release there should be at least one minor release that contains the deprecation so that users can smoothly transition to the new API.

Does SemVer have a size limit on the version string?
No, but use good judgment. A 255 character version string is probably overkill, for example. Also, specific systems may impose their own limits on the size of the string.

Is “v1.2.3” a semantic version?
No, “v1.2.3” is not a semantic version. However, prefixing a semantic version with a “v” is a common way (in English) to indicate it is a version number. Abbreviating “version” as “v” is often seen with version control. Example: git tag v1.2.3 -m "Release version 1.2.3", in which case “v1.2.3” is a tag name and the semantic version is “1.2.3”.

Is there a suggested regular expression (RegEx) to check a SemVer string?
There are two. One with named groups for those systems that support them (PCRE [Perl Compatible Regular Expressions, i.e. Perl, PHP and R], Python and Go).
See: https://regex101.com/r/Ly7O1x/3/
^(?P<major>0|[1-9]\d*)\.(?P<minor>0|[1-9]\d*)\.(?P<patch>0|[1-9]\d*)(?:-(?P<prerelease>(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\.(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\+(?P<buildmetadata>[0-9a-zA-Z-]+(?:\.[0-9a-zA-Z-]+)*))?$
And one with numbered capture groups instead (so cg1 = major, cg2 = minor, cg3 = patch, cg4 = prerelease and cg5 = buildmetadata) that is compatible with ECMA Script (JavaScript), PCRE (Perl Compatible Regular Expressions, i.e. Perl, PHP and R), Python and Go.
See: https://regex101.com/r/vkijKf/1/
^(0|[1-9]\d*)\.(0|[1-9]\d*)\.(0|[1-9]\d*)(?:-((?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\.(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\+([0-9a-zA-Z-]+(?:\.[0-9a-zA-Z-]+)*))?$

About
The Semantic Versioning specification was originally authored by Tom Preston-Werner, inventor of Gravatar and cofounder of GitHub.
If you’d like to leave feedback, please open an issue on GitHub.

License
Creative Commons ― CC BY 3.0

