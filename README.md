# HTTP-HTTP-Hypertext-Transfer-Protocol-is-the-standard-for-communication-across-the-web.-See-topic
Fuel Cells - Hydrogen Fuel Cell Description &amp; Advantages ... https://www.hydrogenics.com/.../fuel-cells DefinitionFunctionsAppearanceMechanismDetailsApplications A fuel cell is a device that converts chemical potential energy (energy stored in molecular bonds) into electrical energy. A PEM (Proton Exchange Membrane) cell uses hydrogen gas (H2) and oxygen gas (O2) as fuel. The products of the reaction in the cell are water, electricity, and heat. This is a big improvement over internal combustion engines, coal burning power plants, and nuclear power plants, all of which produce harmful by-products. See more on hydrogenics.com
Hydrogen + Oxygen = Electricity + Water Vapor

Cathode: O2 + 4H+ + 4e– → 2H2O
Anode: 2H2 → 4H+ + 4e–
Overall: 2H2 + O2 → 2H2O
A fuel cell is a device that converts chemical potential energy (energy stored in molecular bonds) into electrical
Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 

















@@ -150,7 +150,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -166,7 +166,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -179,7 +179,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 



@@ -194,7 +194,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 



@@ -205,7 +205,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





@@ -233,7 +233,7 @@ <h2 id='experiment-4'>Experiment 4: Rotating the perceptive field</h2>





<p><figure> 





<p><figure> 





    <img src="figures/rotation.png" style="width: 448px"> 





    <img src="figures/rotation.png" style="width: 448px"> 





  <figcaption> 





  <figcaption> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 



@@ -258,41 +258,45 @@ <h3>Neural Networks and Self-Organisation</h3>





<h3>Swarm robotics</h3> 





<h3>Swarm robotics</h3> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<h2 id='discussion'>Discussion</h2> 





<h2 id='discussion'>Discussion</h2> 





<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts.</p> 





<h3>Embryogenetic modeling</h3> 















<p><figure> 





<p><figure> 





    <video loop autoplay playsinline muted> 





    <video loop autoplay playsinline muted> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Regeneration-capable 2-headed planarian <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite>, the creature that inspired this work. 





Regeneration-capable 2-headed planarian, the creature that inspired this work <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite> 





<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 









<h3>Engineering and machine learning</h3> 









<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts. We therefore conjecture this kind of modeling may be used for designing reliable, self-organising agents. On the more theoretical machine learning front, we show an instance of a decentralized model able to accomplish remarkably complex tasks. We believe this direction to be opposite to the more traditional global modeling used in the majority of contemporary work in the deep learning field, and we hope this work to be an inspiration to explore more decentralized learning modeling.</p> 





</d-article> 





</d-article> 





<d-appendix> 





<d-appendix> 





<h3>Acknowledgments</h3> 





<h3>Acknowledgments</h3> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 

















<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<h3>Author Contributions</h3> 





<h3>Author Contributions</h3> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and supervised Ettore, who prototyped the model and designed the training regime for the first versions of the experiments. Finally, Alex added some key contributions to the model and the training regime, and performed the final versions of the experiments, shown in the article.</p> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Ettore designed and performed most of the experiments for this work. Alex supervisioned the entire process and contributed extensively to the later stages of development by performing experiments and refining the model.</p> 

















<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. The actual work was triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange with him.</p> 





<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. Alex’s model proposal and this work were triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange between Alex and Micheal.</p> 

















<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 





<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 

















<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<h3>Implementation details</h3> 





<h3>Implementation details</h3> 





<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 



















<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 





<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 

















<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 





<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 

















<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 





<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 



















<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 















<d-footnote-list></d-footnote-list> 





<d-footnote-list></d-footnote-list> 





<d-citation-list></d-citation-list> 





<d-citation-list></d-citation-list> 





<d-appendix> 





<d-appendix> 

1 contents.html 













@@ -9,7 +9,6 @@ <h3>Contents</h3>





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





    </ul> 





    </ul> 





    <div><a href="#implementation">Implementation</a></div> 









    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





  </nav> 





  </nav> 





10 demo.html 













@@ -230,6 +230,10 @@





  width: 200px; 





  width: 200px; 





  height: 16px; 





  height: 16px; 





} 





} 



















.hint a { 









  font-size: 90%; 









} 





</style> 





</style> 

















<div class="l-body-outset grid" id="demo"> 





<div class="l-body-outset grid" id="demo"> 





@@ -268,8 +272,8 @@





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





        </div> 





        </div> 





        <div> 





        <div style="white-space: nowrap;"> 





            Rotation <span id="rotationLabel"></span><span class="hint">&nbsp;&nbsp;<a href="#experiment-4">(experiment 4)</a></span> 





            Rotation <span id="rotationLabel"></span>&nbsp;<span class="hint"><a href="#experiment-4">(experiment 4)</a></span> 





            <br> 





            <br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





        </div> 





        </div> 



@@ -294,7 +298,7 @@





            </span> 





            </span> 





        </div> 





        </div> 





        <div id="colab-hero-div"> 





        <div id="colab-hero-div"> 





          <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





        </div> 





        </div> 





    </div> 





    </div> 





</div> 





</div> 





43 public/index.html 













@@ -453,6 +453,10 @@ <h1>Growing Neural Cellular Automata</h1>





  width: 200px; 





  width: 200px; 





  height: 16px; 





  height: 16px; 





} 





} 



















.hint a { 









  font-size: 90%; 









} 





</style> 





</style> 

















<div class="l-body-outset grid" id="demo"> 





<div class="l-body-outset grid" id="demo"> 





@@ -491,8 +495,8 @@ <h1>Growing Neural Cellular Automata</h1>





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br> 





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         





        </div> 





        </div> 





        <div> 





        <div style="white-space: nowrap;"> 





            Rotation <span id="rotationLabel"></span><span class="hint">&nbsp;&nbsp;<a href="#experiment-4">(experiment 4)</a></span> 





            Rotation <span id="rotationLabel"></span>&nbsp;<span class="hint"><a href="#experiment-4">(experiment 4)</a></span> 





            <br> 





            <br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br> 





        </div> 





        </div> 



@@ -517,7 +521,7 @@ <h1>Growing Neural Cellular Automata</h1>





            </span> 





            </span> 





        </div> 





        </div> 





        <div id="colab-hero-div"> 





        <div id="colab-hero-div"> 





          <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a> 





        </div> 





        </div> 





    </div> 





    </div> 





</div> 





</div> 



@@ -544,7 +548,6 @@ <h3>Contents</h3>





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-3">Learning to regenerate</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





      <li><a href="#experiment-4">Rotating the perceptive field</a></li> 





    </ul> 





    </ul> 





    <div><a href="#implementation">Implementation</a></div> 









    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#related-work">Related Work</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





    <div><a href="#discussion">Discussion</a></div> 





  </nav> 





  </nav> 





@@ -663,7 +666,7 @@ <h2 id='experiment-1'>Experiment 1: Learning to Grow</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 

















@@ -699,7 +702,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -715,7 +718,7 @@ <h2 id='experiment-2'>Experiment 2: What persists, exists</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 















@@ -728,7 +731,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 





<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p> 



@@ -743,7 +746,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure></p> 





</figure></p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 





<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p> 



@@ -754,7 +757,7 @@ <h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





@@ -782,7 +785,7 @@ <h2 id='experiment-4'>Experiment 4: Rotating the perceptive field</h2>





<p><figure> 





<p><figure> 





    <img src="figures/rotation.png" style="width: 448px"> 





    <img src="figures/rotation.png" style="width: 448px"> 





  <figcaption> 





  <figcaption> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 



@@ -807,41 +810,45 @@ <h3>Neural Networks and Self-Organisation</h3>





<h3>Swarm robotics</h3> 





<h3>Swarm robotics</h3> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p> 





<h2 id='discussion'>Discussion</h2> 





<h2 id='discussion'>Discussion</h2> 





<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts.</p> 





<h3>Embryogenetic modeling</h3> 















<p><figure> 





<p><figure> 





    <video loop autoplay playsinline muted> 





    <video loop autoplay playsinline muted> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      <source src="figures/planarian.mp4" type="video/mp4"> 





      Your browser does not support the video tag. 





      Your browser does not support the video tag. 





    </video> 





    </video> 





  <figcaption> 





  <figcaption> 





Regeneration-capable 2-headed planarian <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite>, the creature that inspired this work. 





Regeneration-capable 2-headed planarian, the creature that inspired this work <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite> 





<br><br> <a href="https://colab.research.google.com/drive/1oqVn8ReE7vp77nQLSWAY3v3O_0y-n87Q#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a> 





  </figcaption> 





  </figcaption> 





</figure> 





</figure> 





</p> 





</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 





<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hardcoded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown below). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p> 









<h3>Engineering and machine learning</h3> 









<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts. We therefore conjecture this kind of modeling may be used for designing reliable, self-organising agents. On the more theoretical machine learning front, we show an instance of a decentralized model able to accomplish remarkably complex tasks. We believe this direction to be opposite to the more traditional global modeling used in the majority of contemporary work in the deep learning field, and we hope this work to be an inspiration to explore more decentralized learning modeling.</p> 





</d-article> 





</d-article> 





<d-appendix> 





<d-appendix> 





<h3>Acknowledgments</h3> 





<h3>Acknowledgments</h3> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 





<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p> 

















<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p> 





<h3>Author Contributions</h3> 





<h3>Author Contributions</h3> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and supervised Ettore, who prototyped the model and designed the training regime for the first versions of the experiments. Finally, Alex added some key contributions to the model and the training regime, and performed the final versions of the experiments, shown in the article.</p> 





<p><strong>Research:</strong> Alex came up with the Self-Organizing Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Ettore designed and performed most of the experiments for this work. Alex supervisioned the entire process and contributed extensively to the later stages of development by performing experiments and refining the model.</p> 

















<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. The actual work was triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange with him.</p> 





<p>The idea of applicability of neural networks for understanding regeneration and designing self-organising systems was proposed by Michael Levin in his email to Alex Mordvintsev, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alex in 2015. Alex’s model proposal and this work were triggered by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018, and the subsequent email exchange between Alex and Micheal.</p> 

















<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 





<p><strong>Demos:</strong> Alex created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p> 

















<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<p><strong>Writing & Diagrams:</strong> Alex outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p> 





<h3>Implementation details</h3> 





<h3>Implementation details</h3> 





<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 



















<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 





<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p> 

















<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 





<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p> 

















<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 





<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p> 



















<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p> 















<d-footnote-list></d-footnote-list> 





<d-footnote-list></d-footnote-list> 





<d-citation-list></d-citation-list> 





<d-citation-list></d-citation-list> 





<d-appendix> 



